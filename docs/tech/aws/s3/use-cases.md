# S3æ´»ç”¨äº‹ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

!!! info "ã“ã®ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦"
    S3ã®å®Ÿéš›ã®æ´»ç”¨äº‹ä¾‹ã¨ã€ãã‚Œãã‚Œã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–æ‰‹æ³•ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## é™çš„ã‚µã‚¤ãƒˆãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°

### åŸºæœ¬è¨­å®š

```bash
# é™çš„ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®æœ‰åŠ¹åŒ–
aws s3api put-bucket-website \
    --bucket my-website-bucket \
    --website-configuration '{
        "IndexDocument": {"Suffix": "index.html"},
        "ErrorDocument": {"Key": "error.html"}
    }'

# ã‚µã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
aws s3 sync ./website/ s3://my-website-bucket/ \
    --delete \
    --cache-control "max-age=86400"
```

### Next.js / React SPAå¯¾å¿œ

**Next.js Static Exportè¨­å®š**

```javascript
// next.config.js
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: 'export',
  trailingSlash: true,
  images: {
    unoptimized: true
  }
}

module.exports = nextConfig
```

**ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**

```bash
#!/bin/bash

# Next.jsãƒ“ãƒ«ãƒ‰
npm run build

# S3ã«ãƒ‡ãƒ—ãƒ­ã‚¤
aws s3 sync ./out/ s3://my-nextjs-site/ \
    --delete \
    --cache-control "public, max-age=31536000" \
    --exclude "*.html" \
    --exclude "sw.js"

# HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¯çŸ­ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“
aws s3 sync ./out/ s3://my-nextjs-site/ \
    --cache-control "public, max-age=0, must-revalidate" \
    --include "*.html" \
    --include "sw.js"

echo "Deployment completed!"
```

!!! warning "SPAãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å¯¾å¿œ"
    React Routerã‚„Next.js Routerä½¿ç”¨æ™‚ã¯ã€ã™ã¹ã¦ã®404ã‚¨ãƒ©ãƒ¼ã‚’`index.html`ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã™ã‚‹è¨­å®šãŒå¿…è¦ã§ã™ã€‚S3å˜ä½“ã§ã¯å›°é›£ãªãŸã‚ã€CloudFrontã¨ã®çµ„ã¿åˆã‚ã›ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡

### ç”»åƒãƒ»å‹•ç”»é…ä¿¡ã®æœ€é©åŒ–

**ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šä¾‹**

```json
{
    "Rules": [
        {
            "ID": "MediaOptimization",
            "Status": "Enabled",
            "Filter": {
                "Prefix": "media/"
            },
            "Transitions": [
                {
                    "Days": 90,
                    "StorageClass": "STANDARD_IA"
                },
                {
                    "Days": 365,
                    "StorageClass": "GLACIER_IR"
                }
            ]
        }
    ]
}
```

**ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨Lambdaé–¢æ•°**

```python
import json
import boto3
from PIL import Image
import io

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    
    # å…ƒã®ç”»åƒã‚’å–å¾—
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # ã‚µãƒ ãƒã‚¤ãƒ«ç”Ÿæˆ
    response = s3.get_object(Bucket=bucket, Key=key)
    image = Image.open(response['Body'])
    
    # ãƒªã‚µã‚¤ã‚ºï¼ˆè¤‡æ•°ã‚µã‚¤ã‚ºï¼‰
    sizes = [150, 300, 600]
    for size in sizes:
        thumbnail = image.copy()
        thumbnail.thumbnail((size, size), Image.Resampling.LANCZOS)
        
        # WebPå½¢å¼ã§ä¿å­˜
        output = io.BytesIO()
        thumbnail.save(output, format='WebP', quality=85)
        
        s3.put_object(
            Bucket=bucket,
            Key=f"thumbnails/{size}px/{key.replace('.jpg', '.webp')}",
            Body=output.getvalue(),
            ContentType='image/webp',
            CacheControl='public, max-age=31536000'
        )
    
    return {'statusCode': 200}
```

### å‹•ç”»é…ä¿¡ã®æœ€é©åŒ–

```bash
# HLSå½¢å¼ã®å‹•ç”»é…ä¿¡ç”¨ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ 
# videos/
#   â”œâ”€â”€ video-001/
#   â”‚   â”œâ”€â”€ master.m3u8
#   â”‚   â”œâ”€â”€ 720p/
#   â”‚   â”‚   â”œâ”€â”€ playlist.m3u8
#   â”‚   â”‚   â””â”€â”€ segment-*.ts
#   â”‚   â””â”€â”€ 1080p/
#   â”‚       â”œâ”€â”€ playlist.m3u8
#   â”‚       â””â”€â”€ segment-*.ts

# å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
aws s3 cp video.mp4 s3://my-media-bucket/raw-videos/ \
    --metadata "processing-status=pending"

# HLSã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆé©åˆ‡ãªContent-Typeè¨­å®šï¼‰
aws s3 sync ./hls-output/ s3://my-media-bucket/videos/video-001/ \
    --exclude "*" \
    --include "*.m3u8" \
    --content-type "application/x-mpegURL" \
    --cache-control "public, max-age=10"

aws s3 sync ./hls-output/ s3://my-media-bucket/videos/video-001/ \
    --exclude "*" \
    --include "*.ts" \
    --content-type "video/MP2T" \
    --cache-control "public, max-age=31536000"
```

## ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

**MySQL/PostgreSQLè‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**

```bash
#!/bin/bash

DB_NAME="production_db"
DB_USER="backup_user"
BUCKET="my-backup-bucket"
PREFIX="database-backups/$(date +%Y/%m/%d)"

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ãƒ³ãƒ—ä½œæˆ
mysqldump -u $DB_USER -p$DB_PASS $DB_NAME | gzip > db_backup_$(date +%Y%m%d_%H%M%S).sql.gz

# S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆGlacierå³åº§å–å¾—ã‚¯ãƒ©ã‚¹ä½¿ç”¨ï¼‰
aws s3 cp db_backup_*.sql.gz s3://$BUCKET/$PREFIX/ \
    --storage-class GLACIER_IR

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
rm db_backup_*.sql.gz

# å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šï¼‰
aws s3api list-objects-v2 \
    --bucket $BUCKET \
    --prefix database-backups/ \
    --query "Contents[?LastModified<='$(date -d '30 days ago' --iso-8601)'].Key" \
    --output text | xargs -I {} aws s3 rm s3://$BUCKET/{}

echo "Backup completed: s3://$BUCKET/$PREFIX/"
```

### ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

```bash
#!/bin/bash

# é‡è¦ãªã‚·ã‚¹ãƒ†ãƒ è¨­å®šã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
BACKUP_DATE=$(date +%Y%m%d)
BUCKET="my-system-backup"

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
tar -czf system_config_$BACKUP_DATE.tar.gz \
    /etc/nginx/ \
    /etc/apache2/ \
    /etc/ssl/ \
    /home/user/.ssh/ \
    --exclude="*.log"

# Deep Archiveã«ä¿å­˜ï¼ˆé•·æœŸä¿å­˜ãƒ»ä½ã‚³ã‚¹ãƒˆï¼‰
aws s3 cp system_config_$BACKUP_DATE.tar.gz \
    s3://$BUCKET/system-configs/ \
    --storage-class DEEP_ARCHIVE

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
tar -czf app_code_$BACKUP_DATE.tar.gz /var/www/html/ --exclude="node_modules"
aws s3 cp app_code_$BACKUP_DATE.tar.gz \
    s3://$BUCKET/application-code/ \
    --storage-class STANDARD_IA

rm system_config_$BACKUP_DATE.tar.gz app_code_$BACKUP_DATE.tar.gz
```

## ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯æ§‹ç¯‰

### ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ä¿å­˜

**Fluent Bitè¨­å®šä¾‹**

```ini
[SERVICE]
    Flush 60
    Daemon off
    Log_Level info

[INPUT]
    Name tail
    Path /var/log/nginx/access.log
    Tag nginx.access

[OUTPUT]
    Name s3
    Match nginx.*
    bucket my-datalake-bucket
    region ap-northeast-1
    s3_key_format /logs/nginx/year=%Y/month=%m/day=%d/nginx-access-%Y%m%d-%H%M%S.log
    total_file_size 50MB
    upload_timeout 10m
    storage_class STANDARD_IA
```

### Athenaã§ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ

**ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ§‹é€ ã®è¨­è¨ˆ**

```
my-datalake-bucket/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ nginx/
â”‚   â”‚   â”œâ”€â”€ year=2025/month=02/day=13/
â”‚   â”‚   â””â”€â”€ year=2025/month=02/day=14/
â”‚   â””â”€â”€ application/
â”‚       â”œâ”€â”€ year=2025/month=02/day=13/
â”‚       â””â”€â”€ year=2025/month=02/day=14/
â””â”€â”€ analytics/
    â”œâ”€â”€ daily-summary/
    â””â”€â”€ monthly-reports/
```

**Athenaãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆSQL**

```sql
CREATE EXTERNAL TABLE nginx_logs (
    timestamp string,
    method string,
    url string,
    status_code int,
    response_time float,
    user_agent string
)
PARTITIONED BY (
    year string,
    month string,
    day string
)
STORED AS PARQUET
LOCATION 's3://my-datalake-bucket/logs/nginx/';

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è‡ªå‹•ç™ºè¦‹ã®æœ‰åŠ¹åŒ–
MSCK REPAIR TABLE nginx_logs;
```

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**AWS Glue Jobã«ã‚ˆã‚‹å®šæœŸå‡¦ç†**

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ç”Ÿãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿
raw_logs = glueContext.create_dynamic_frame.from_options(
    "s3",
    {"paths": ["s3://my-datalake-bucket/logs/nginx/"]},
    format="json"
)

# ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
transformed_logs = raw_logs.map(lambda x: {
    'timestamp': x['timestamp'],
    'method': x['method'].upper(),
    'status_code': int(x['status_code']),
    'response_time': float(x['response_time']),
    'date': x['timestamp'][:10]  # YYYY-MM-DD
})

# Parquetå½¢å¼ã§å‡ºåŠ›
glueContext.write_dynamic_frame.from_options(
    transformed_logs,
    "s3",
    {
        "path": "s3://my-datalake-bucket/processed-logs/",
        "partitionKeys": ["date"]
    },
    format="parquet"
)

job.commit()
```

## ãƒ­ã‚°ä¿å­˜

### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã®è‡ªå‹•åé›†

**CloudWatch Logs â†’ S3ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**

```bash
# ãƒ­ã‚°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’S3ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
aws logs create-export-task \
    --log-group-name "/aws/lambda/my-function" \
    --from-time $(date -d '1 day ago' +%s)000 \
    --to-time $(date +%s)000 \
    --destination "my-logs-bucket" \
    --destination-prefix "lambda-logs/$(date +%Y/%m/%d)/"
```

**ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨Lambda**

```python
import boto3
import json
from datetime import datetime, timedelta

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    logs = boto3.client('logs')
    
    # 1æ—¥å‰ã®ãƒ­ã‚°ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    yesterday = datetime.now() - timedelta(days=1)
    from_time = int(yesterday.replace(hour=0, minute=0, second=0).timestamp() * 1000)
    to_time = int(yesterday.replace(hour=23, minute=59, second=59).timestamp() * 1000)
    
    log_groups = [
        '/aws/lambda/api-handler',
        '/aws/lambda/data-processor',
        '/aws/ecs/my-application'
    ]
    
    for log_group in log_groups:
        try:
            response = logs.create_export_task(
                logGroupName=log_group,
                fromTime=from_time,
                toTime=to_time,
                destination='my-logs-archive-bucket',
                destinationPrefix=f"application-logs/{yesterday.strftime('%Y/%m/%d')}/{log_group.replace('/', '_')}"
            )
            print(f"Export task created for {log_group}: {response['taskId']}")
        except Exception as e:
            print(f"Failed to export {log_group}: {str(e)}")
    
    return {'statusCode': 200, 'body': 'Log export tasks created'}
```

## CloudFronté€£æº

### OACï¼ˆOrigin Access Controlï¼‰è¨­å®š

!!! tip "OAC vs OAI"
    2025å¹´ç¾åœ¨ã€OACï¼ˆOrigin Access Controlï¼‰ãŒOAIï¼ˆOrigin Access Identityï¼‰ã®å¾Œç¶™ã¨ã—ã¦æ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚OACã¯å…¨ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œã€ç½²åãƒãƒ¼ã‚¸ãƒ§ãƒ³4ã‚µãƒãƒ¼ãƒˆãªã©ã®åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚

**CloudFrontãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ**

```bash
# OACä½œæˆ
aws cloudfront create-origin-access-control \
    --origin-access-control-config '{
        "Name": "my-s3-oac",
        "Description": "OAC for my S3 bucket",
        "OriginAccessControlOriginType": "s3",
        "SigningBehavior": "always",
        "SigningProtocol": "sigv4"
    }'

# ãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
cat > distribution-config.json << 'EOF'
{
    "CallerReference": "my-distribution-2025-02-13",
    "Comment": "S3 + CloudFront distribution",
    "DefaultCacheBehavior": {
        "TargetOriginId": "my-s3-origin",
        "ViewerProtocolPolicy": "redirect-to-https",
        "AllowedMethods": {
            "Quantity": 2,
            "Items": ["GET", "HEAD"]
        },
        "ForwardedValues": {
            "QueryString": false,
            "Cookies": {"Forward": "none"}
        },
        "Compress": true,
        "CachePolicyId": "4135ea2d-6df8-44a3-9df3-4b5a84be39ad"
    },
    "Origins": {
        "Quantity": 1,
        "Items": [
            {
                "Id": "my-s3-origin",
                "DomainName": "my-static-site-bucket.s3.ap-northeast-1.amazonaws.com",
                "OriginAccessControlId": "OAC_ID_HERE",
                "S3OriginConfig": {
                    "OriginAccessIdentity": ""
                }
            }
        ]
    },
    "Enabled": true
}
EOF
```

**S3ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ï¼ˆOACç”¨ï¼‰**

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "cloudfront.amazonaws.com"
            },
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::my-static-site-bucket/*",
            "Condition": {
                "StringEquals": {
                    "AWS:SourceArn": "arn:aws:cloudfront::123456789012:distribution/E1234567890123"
                }
            }
        }
    ]
}
```

### é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š

**è¤‡æ•°ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ“ãƒ˜ã‚¤ãƒ“ã‚¢**

```json
{
    "CacheBehaviors": {
        "Quantity": 3,
        "Items": [
            {
                "PathPattern": "/api/*",
                "TargetOriginId": "api-origin",
                "ViewerProtocolPolicy": "https-only",
                "CachePolicyId": "4135ea2d-6df8-44a3-9df3-4b5a84be39ad",
                "TTL": {
                    "DefaultTTL": 0,
                    "MaxTTL": 0,
                    "MinTTL": 0
                }
            },
            {
                "PathPattern": "/static/*",
                "TargetOriginId": "my-s3-origin",
                "ViewerProtocolPolicy": "https-only",
                "CachePolicyId": "658327ea-f89d-4fab-a63d-7e88639e58f6",
                "TTL": {
                    "DefaultTTL": 31536000,
                    "MaxTTL": 31536000,
                    "MinTTL": 0
                }
            }
        ]
    }
}
```

## ã‚³ã‚¹ãƒˆæœ€é©åŒ–

### Intelligent-Tieringæ´»ç”¨

```bash
# ãƒã‚±ãƒƒãƒˆå…¨ä½“ã§Intelligent-Tieringæœ‰åŠ¹åŒ–
aws s3api put-bucket-intelligent-tiering-configuration \
    --bucket my-app-storage-20250213 \
    --id "EntireBucket" \
    --intelligent-tiering-configuration '{
        "Id": "EntireBucket",
        "Status": "Enabled",
        "Filter": {},
        "Tierings": [
            {
                "Days": 90,
                "AccessTier": "ARCHIVE_ACCESS"
            },
            {
                "Days": 180,
                "AccessTier": "DEEP_ARCHIVE_ACCESS"
            }
        ]
    }'

# ç‰¹å®šã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ã¿å¯¾è±¡
aws s3api put-bucket-intelligent-tiering-configuration \
    --bucket my-app-storage-20250213 \
    --id "DocumentsOnly" \
    --intelligent-tiering-configuration '{
        "Id": "DocumentsOnly",
        "Status": "Enabled",
        "Filter": {
            "Prefix": "documents/"
        },
        "Tierings": [
            {
                "Days": 90,
                "AccessTier": "ARCHIVE_ACCESS"
            }
        ]
    }'
```

### ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ã®ç›£è¦–

**CloudWatch ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—**

```bash
# ãƒã‚±ãƒƒãƒˆä½¿ç”¨é‡å–å¾—
aws cloudwatch get-metric-statistics \
    --namespace AWS/S3 \
    --metric-name BucketSizeBytes \
    --dimensions Name=BucketName,Value=my-app-storage-20250213 Name=StorageType,Value=StandardStorage \
    --statistics Sum \
    --start-time $(date -d '7 days ago' --iso-8601) \
    --end-time $(date --iso-8601) \
    --period 86400

# ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ•°å–å¾—
aws cloudwatch get-metric-statistics \
    --namespace AWS/S3 \
    --metric-name NumberOfObjects \
    --dimensions Name=BucketName,Value=my-app-storage-20250213 Name=StorageType,Value=AllStorageTypes \
    --statistics Average \
    --start-time $(date -d '7 days ago' --iso-8601) \
    --end-time $(date --iso-8601) \
    --period 86400
```

**ã‚³ã‚¹ãƒˆåˆ†æç”¨Lambda**

```python
import boto3
import json
from datetime import datetime, timedelta

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    cloudwatch = boto3.client('cloudwatch')
    
    bucket_name = 'my-app-storage-20250213'
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹åˆ¥ã®ä½¿ç”¨é‡ã‚’å–å¾—
    storage_classes = ['StandardStorage', 'StandardIAStorage', 'GlacierIRStorage', 'GlacierStorage', 'DeepArchiveStorage']
    
    cost_summary = {}
    
    for storage_class in storage_classes:
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/S3',
            MetricName='BucketSizeBytes',
            Dimensions=[
                {'Name': 'BucketName', 'Value': bucket_name},
                {'Name': 'StorageType', 'Value': storage_class}
            ],
            StartTime=datetime.now() - timedelta(days=1),
            EndTime=datetime.now(),
            Period=86400,
            Statistics=['Average']
        )
        
        if response['Datapoints']:
            size_bytes = response['Datapoints'][0]['Average']
            size_gb = size_bytes / (1024**3)
            cost_summary[storage_class] = round(size_gb, 2)
    
    # Slackã«é€ä¿¡
    webhook_url = 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
    
    message = f"ğŸ“Š S3 Storage Usage Report - {bucket_name}\n"
    total_size = 0
    for storage_class, size in cost_summary.items():
        message += f"â€¢ {storage_class}: {size} GB\n"
        total_size += size
    message += f"**Total: {total_size} GB**"
    
    return {
        'statusCode': 200,
        'body': json.dumps(cost_summary)
    }
```

### ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–™é‡‘ã®æœ€é©åŒ–

```bash
# S3 Transfer Accelerationã®æœ‰åŠ¹åŒ–ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ã‚¯ã‚»ã‚¹æ™‚ï¼‰
aws s3api put-bucket-accelerate-configuration \
    --bucket my-app-storage-20250213 \
    --accelerate-configuration Status=Enabled

# Transfer AccelerationçµŒç”±ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
aws s3 cp large-file.zip s3://my-app-storage-20250213/ \
    --endpoint-url https://s3-accelerate.amazonaws.com

# ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¾å€¤ã®èª¿æ•´
aws configure set default.s3.multipart_threshold 64MB
aws configure set default.s3.max_concurrent_requests 10
```

!!! tip "ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹"
    1. **ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è‡ªå‹•åŒ–**: ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿œã˜ãŸã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹ç§»è¡Œ
    2. **Intelligent-Tiering**: ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¸æ˜ãªå ´åˆã®è‡ªå‹•æœ€é©åŒ–
    3. **ä¸å®Œå…¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‰Šé™¤**: ä¸­æ–­ã•ã‚ŒãŸãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    4. **CloudWatchç›£è¦–**: å®šæœŸçš„ãªä½¿ç”¨é‡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ç•°å¸¸æ¤œçŸ¥
    5. **åœ§ç¸®**: å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯äº‹å‰åœ§ç¸®ã—ã¦ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å‰Šæ¸›

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã®è¨­å®š

```bash
# ã‚µãƒ¼ãƒãƒ¼ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–
aws s3api put-bucket-logging \
    --bucket my-app-storage-20250213 \
    --bucket-logging-status '{
        "LoggingEnabled": {
            "TargetBucket": "my-access-logs-bucket",
            "TargetPrefix": "s3-access-logs/my-app-storage-20250213/"
        }
    }'
```

### VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã‚¢ã‚¯ã‚»ã‚¹

```bash
# VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½œæˆ
aws ec2 create-vpc-endpoint \
    --vpc-id vpc-12345678 \
    --service-name com.amazonaws.ap-northeast-1.s3 \
    --route-table-ids rtb-12345678

# VPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã§ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã™ã‚‹ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼
cat > vpc-only-policy.json << 'EOF'
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::my-secure-bucket",
                "arn:aws:s3:::my-secure-bucket/*"
            ],
            "Condition": {
                "StringNotEquals": {
                    "aws:sourceVpce": "vpce-12345678"
                }
            }
        }
    ]
}
EOF
```

### ç½²åä»˜ãURLç”Ÿæˆ

```python
import boto3
from botocore.exceptions import ClientError
from datetime import timedelta

def generate_presigned_url(bucket_name, object_key, expiration=3600):
    """ç½²åä»˜ãURLç”Ÿæˆ"""
    s3_client = boto3.client('s3')
    
    try:
        response = s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': bucket_name, 'Key': object_key},
            ExpiresIn=expiration
        )
        return response
    except ClientError as e:
        return None

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ç½²åä»˜ãURL
def generate_presigned_post(bucket_name, object_key, expiration=3600):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ç½²åä»˜ãURLç”Ÿæˆ"""
    s3_client = boto3.client('s3')
    
    try:
        response = s3_client.generate_presigned_post(
            Bucket=bucket_name,
            Key=object_key,
            Fields={'Content-Type': 'image/jpeg'},
            Conditions=[
                {'Content-Type': 'image/jpeg'},
                ['content-length-range', 1, 10*1024*1024]  # 1B-10MB
            ],
            ExpiresIn=expiration
        )
        return response
    except ClientError as e:
        return None
```

!!! success "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—"
    S3ã«ã¤ã„ã¦ç†è§£ã§ããŸã‚‰ã€[CloudFrontã¨ã®çµ„ã¿åˆã‚ã›](../cloudfront/index.md)ã§ã•ã‚‰ã«é«˜é€ŸåŒ–ãƒ»ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚’å›³ã‚Šã¾ã—ã‚‡ã†ã€‚

## é–¢é€£ãƒªãƒ³ã‚¯

- [S3ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰](setup.md)
- [CloudFrontæ¦‚è¦](../cloudfront/index.md)
- [AWS S3 æ–™é‡‘è¨ˆç®—ãƒ„ãƒ¼ãƒ«](https://calculator.aws/)